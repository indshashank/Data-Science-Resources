{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###High level summary\n",
    "Train a series of weak decision tree learners, with each successive tree attacking the residual from all previous trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Algorithm\n",
    "Train a weak decision tree $F^0$ to fit the data\n",
    "$$\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "F^0(\\vec{x_1}) \\qquad y_1 \\\\\n",
    "F^0(\\vec{x_2}) \\qquad y_2 \\\\\n",
    "\\vdots \\qquad \\vdots \\\\\n",
    "F^0(\\vec{x_n}) \\qquad y_n$$\n",
    "\n",
    "This will lead to residuals\n",
    "$$r^0(\\vec{x_1}) = y_1 - F^0(\\vec{x_1})\\\\ r^0(\\vec{x_2}) = y_2 - F^0(\\vec{x_2})\\\\ \\vdots \\\\r^0(\\vec{x_n}) = y_n - F^0(\\vec{x_n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit a second tree $F^1$ to the residual so that the new overall prediction $F(X) = F^0(X) + F^1(X)$\n",
    "\n",
    "$$\n",
    "F^1(\\vec{x_1}) \\qquad y_1 - F^0(\\vec{x_1}) \\\\\n",
    "F^1(\\vec{x_2}) \\qquad y_2 - F^0(\\vec{x_2}) \\\\\n",
    "\\vdots \\qquad \\vdots \\\\\n",
    "F^1(\\vec{x_n}) \\qquad y_n - F^0(\\vec{x_n})$$\n",
    "\n",
    "This will lead to residuals\n",
    "$$r^1(\\vec{x_1}) = y_1 - \\left(F^0(\\vec{x_1}) + F^1(\\vec{x_1})\\right)\\\\ r^1(\\vec{x_2}) = y_2 - \\left(F^0(\\vec{x_2}) + F^1(\\vec{x_2})\\right)\\\\ \\vdots \\\\r^1(\\vec{x_n}) = y_n - \\left(F^0(\\vec{x_n}) + F^1(\\vec{x_n})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep fitting decision trees until some criterion is met or max trees have been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok, but why is this called gradient boosting?**\n",
    "\n",
    "Consider the loss function\n",
    "$$L(y, F(\\vec{x})) = \\frac{(y-F(\\vec{x}))^2}{2}$$\n",
    "\n",
    "Which leads to the cost function\n",
    "$$J = \\sum_i L(y_i, F(\\vec{x_i}))$$\n",
    "\n",
    "The negative gradient of this cost function is\n",
    "$$-\\frac{\\partial J}{\\partial{F(\\vec{x_i})}} = y_i - F(\\vec{x_i})$$\n",
    "\n",
    "which is exactly the residual used in the boosting algorithm described above. The residuals are really just the negative gradients of the cost function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other loss functions\n",
    "\n",
    "#### Absolute loss\n",
    "\n",
    "\n",
    "$$L(y, F) = \\left|y-F\\right| \\\\ -g(\\vec{x_i}) = -\\frac{\\partial J}{\\partial{F(\\vec{x_i})}} = sign(y_i - F(\\vec{x_i}))$$\n",
    "\n",
    "#### Huber loss\n",
    "\n",
    "\n",
    "$$\n",
    "L(y, F) =\\begin{cases}\\frac{(y-F)^2}{2} \\qquad \\left|y-F\\right| \\leq \\delta, \\\\ \\delta(\\left|y-F\\right| - \\frac{\\delta}{2} \\qquad\n",
    "\\left|y-F\\right| \\gt \\delta \\end{cases} \\\\\n",
    "-g(\\vec{x_i}) = -\\frac{\\partial J}{\\partial{F(\\vec{x_i})}} = \\begin{cases} y_i-F(\\vec{x_i}) \\qquad \\left|y_i-F(\\vec{x_i})\\right| \\leq \\delta, \\\\ \\delta \\cdot sign(\\left|y-F\\right| \\qquad\n",
    "\\left|y_i-F(\\vec{x_i})\\right| \\gt \\delta \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
