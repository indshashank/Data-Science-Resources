{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###High level summary\n",
    "Train a series of weak decision tree learners, with each successive tree attacking the residual from all previous trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Algorithm\n",
    "Train a weak decision tree $F^0$ to fit the data\n",
    "$$\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "F^0(\\vec{x_1}) \\qquad y_1 \\\\\n",
    "F^0(\\vec{x_2}) \\qquad y_2 \\\\\n",
    "\\vdots \\qquad \\vdots \\\\\n",
    "F^0(\\vec{x_n}) \\qquad y_n$$\n",
    "\n",
    "This will lead to residuals\n",
    "$$r^0(\\vec{x_1}) = y_1 - F^0(\\vec{x_1})\\\\ r^0(\\vec{x_2}) = y_2 - F^0(\\vec{x_2})\\\\ \\vdots \\\\r^0(\\vec{x_n}) = y_n - F^0(\\vec{x_n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit a second tree $F^1$ to the residual so that the new overall prediction $F(X) = F^0(X) + F^1(X)$\n",
    "\n",
    "$$\n",
    "F^1(\\vec{x_1}) \\qquad y_1 - F^0(\\vec{x_1}) \\\\\n",
    "F^1(\\vec{x_2}) \\qquad y_2 - F^0(\\vec{x_2}) \\\\\n",
    "\\vdots \\qquad \\vdots \\\\\n",
    "F^1(\\vec{x_n}) \\qquad y_n - F^0(\\vec{x_n})$$\n",
    "\n",
    "This will lead to residuals\n",
    "$$r^1(\\vec{x_1}) = y_1 - \\left(F^0(\\vec{x_1}) + F^1(\\vec{x_1})\\right)\\\\ r^1(\\vec{x_2}) = y_2 - \\left(F^0(\\vec{x_2}) + F^1(\\vec{x_2})\\right)\\\\ \\vdots \\\\r^1(\\vec{x_n}) = y_n - \\left(F^0(\\vec{x_n}) + F^1(\\vec{x_n})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep fitting decision trees until some criterion is met or max trees have been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok, but why is this called gradient boosting?**\n",
    "\n",
    "Consider the loss function\n",
    "$$L(y, F(\\vec{x})) = \\frac{(y-F(\\vec{x}))^2}{2}$$\n",
    "\n",
    "Which leads to the cost function\n",
    "$$J = \\sum_i L(y_i, F(\\vec{x_i}))$$\n",
    "\n",
    "The negative gradient of this cost function is\n",
    "$$-\\frac{\\partial J}{\\partial{F(\\vec{x_i})}} = y_i - F(\\vec{x_i})$$\n",
    "\n",
    "which is exactly the residual used in the boosting algorithm described above. The residuals are really just the negative gradients of the cost function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other loss functions\n",
    "\n",
    "#### Absolute loss\n",
    "\n",
    "\n",
    "$$L(y, F) = \\left|y-F\\right| \\\\ -g(\\vec{x_i}) = -\\frac{\\partial J}{\\partial{F(\\vec{x_i})}} = sign(y_i - F(\\vec{x_i}))$$\n",
    "\n",
    "#### Huber loss\n",
    "\n",
    "\n",
    "$$\n",
    "L(y, F) =\\begin{cases}\\frac{(y-F)^2}{2} \\qquad \\left|y-F\\right| \\leq \\delta, \\\\ \\delta(\\left|y-F\\right| - \\frac{\\delta}{2} \\qquad\n",
    "\\left|y-F\\right| \\gt \\delta \\end{cases} \\\\\n",
    "-g(\\vec{x_i}) = -\\frac{\\partial J}{\\partial{F(\\vec{x_i})}} = \\begin{cases} y_i-F(\\vec{x_i}) \\qquad \\left|y_i-F(\\vec{x_i})\\right| \\leq \\delta, \\\\ \\delta \\cdot sign(\\left|y-F\\right| \\qquad\n",
    "\\left|y_i-F(\\vec{x_i})\\right| \\gt \\delta \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Boosting for Classification\n",
    "\n",
    "An simple example will be the best way to convey the method of boosted decision trees for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "We will try to classify a dataset into one of three categories $\\{A, B, C\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x2</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x3</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x4</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x  y\n",
       "0  x0  A\n",
       "1  x1  C\n",
       "2  x2  B\n",
       "3  x3  B\n",
       "4  x4  A"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sample_data = {\"x\": [\"x%d\" % i for i in xrange(5)], \"y\": [\"A\", \"C\", \"B\", \"B\", \"A\"]}\n",
    "df = pd.DataFrame(sample_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the output data into a probability distribution matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "0  1  0  0\n",
       "1  0  0  1\n",
       "2  0  1  0\n",
       "3  0  1  0\n",
       "4  1  0  0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = pd.get_dummies(df.y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each category, we will train a boosted decision tree to predict a score for each data point. A higher score will lead to higher likelihood of predicting that category. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
